<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<title>Project 1: Prokudin-Gorskii Colorization — CS180</title>
<meta name="viewport" content="width=device-width,initial-scale=1">
<style>
    body {
        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
        margin: 0;
        padding: 40px;
        line-height: 1.7;
        background: linear-gradient(135deg, #f5f5f5 0%, #e0e0e0 100%);
        min-height: 100vh;
        color: #333;
    }
    
    .container {
        max-width: 1200px;
        margin: 0 auto;
        background: rgba(255, 255, 255, 0.95);
        border-radius: 20px;
        padding: 60px;
        box-shadow: 0 20px 60px rgba(0, 0, 0, 0.3);
        backdrop-filter: blur(10px);
    }
    
    header {
        display: flex;
        align-items: center;
        gap: 30px;
        margin-bottom: 50px;
        padding-bottom: 30px;
        border-bottom: 3px solid #666;
    }
    
    header img.logo { 
        height: 80px;
        filter: drop-shadow(0 4px 8px rgba(0,0,0,0.2));
    }
    
    .header-text h1 { 
        font-size: 2.5rem; 
        margin: 0; 
        color: #2c3e50;
        font-weight: 700;
        background: linear-gradient(135deg, #666, #999);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
    }
    
    .header-text .subtitle {
        font-size: 1.1rem;
        color: #666;
        margin-top: 8px;
        font-weight: 500;
    }
    
    h2 { 
        margin-top: 50px; 
        margin-bottom: 25px; 
        color: #2c3e50; 
        font-size: 1.8rem;
        font-weight: 600;
        padding: 15px 25px;
        background: linear-gradient(135deg, #f9f9f9, #e9e9e9);
        border-radius: 12px;
        border-left: 5px solid #666;
    }
    
    h3 { 
        margin-top: 35px; 
        color: #444; 
        font-size: 1.3rem;
        font-weight: 600;
        border-bottom: 2px solid #eee;
        padding-bottom: 8px;
    }
    
    h4 {
        color: #555;
        font-size: 1.1rem;
        margin-top: 25px;
        font-weight: 600;
    }
    
    p { 
        color: #444; 
        font-size: 1.05rem;
        margin-bottom: 20px;
        text-align: justify;
    }
    
    ul, ol {
        color: #444;
        font-size: 1.05rem;
        line-height: 1.8;
    }
    
    li {
        margin-bottom: 8px;
    }
    
    .image-row { 
        display: flex; 
        gap: 30px; 
        flex-wrap: wrap; 
        justify-content: center; 
        margin: 40px 0; 
    }
    
    figure { 
        width: 280px; 
        text-align: center; 
        margin: 0;
        background: white;
        border-radius: 15px;
        padding: 20px;
        box-shadow: 0 8px 25px rgba(0,0,0,0.1);
        transition: transform 0.3s ease, box-shadow 0.3s ease;
    }
    
    figure:hover {
        transform: translateY(-5px);
        box-shadow: 0 15px 35px rgba(0,0,0,0.15);
    }
    
    figure img { 
        border-radius: 12px; 
        max-width: 100%; 
        height: auto;
        box-shadow: 0 4px 15px rgba(0,0,0,0.1);
    }
    
    figcaption { 
        font-size: 0.95rem; 
        color: #666; 
        margin-top: 15px;
        font-weight: 500;
    }
    
    pre { 
        background: #f8f9fa; 
        padding: 20px; 
        border-radius: 10px; 
        border: 1px solid #e9ecef; 
        overflow-x: auto;
        font-size: 0.95rem;
        box-shadow: inset 0 2px 4px rgba(0,0,0,0.06);
    }
    
    .formula { 
        display: block; 
        text-align: center; 
        margin: 30px auto; 
        max-width: 600px;
        padding: 20px;
        background: #f8f9fa;
        border-radius: 10px;
        border: 2px solid #e9ecef;
    }
    
    .formula img {
        max-width: 100%;
        height: auto;
    }
    
    hr {
        border: none;
        height: 2px;
        background: linear-gradient(135deg, #666, #999);
        margin: 50px 0;
        border-radius: 2px;
    }
    
    .metric-section {
        background: #f8f9fa;
        padding: 25px;
        border-radius: 12px;
        margin: 25px 0;
        border-left: 4px solid #666;
    }
    
    .highlight-box {
        background: linear-gradient(135deg, #f0f0f0, #ddd);
        padding: 20px;
        border-radius: 12px;
        margin: 25px 0;
        border-left: 5px solid #888;
    }
    
    .highlight-box p {
        margin: 0;
        font-weight: 500;
    }
    
    code {
        background: #f1f3f4;
        padding: 2px 6px;
        border-radius: 4px;
        font-family: 'Courier New', monospace;
        font-size: 0.9rem;
        color: #c7254e;
    }
</style>
</head>
<body>

<div class="container">
<header>
    <div class="header-text">
        <h1>Colorizing the Prokudin-Gorskii Photo Collection</h1>
        <div class="subtitle">CS180: Intro to Computer Vision and Computational Photography — Project #1</div>
    </div>
</header>

<h2>Introduction</h2>
<p>
This project reconstructs color photographs from Sergei Prokudin-Gorskii's digitized glass-plate negatives from the early 1900s. 
Each glass plate contains three monochrome exposures taken through blue, green, and red filters, stacked vertically. 
By carefully extracting these three channels, aligning them with high precision, and combining them into a single RGB image, 
we can automatically produce stunning full-color photographs that bring history to life.
</p>

<p>
The pipeline handles both low-resolution JPEG images (using single-scale alignment) and high-resolution TIF scans 
(using multi-scale pyramid alignment for efficiency). The key challenge is achieving precise alignment despite 
slight camera movements, glass plate imperfections, and varying lighting conditions between exposures.
</p>

<hr>

<h2>Approach Overview</h2>
<p>
The reconstruction process follows a systematic pipeline:
</p>

<ol>
    <li><strong>Channel Extraction:</strong> Split the vertically-stacked BGR image into three separate grayscale channels</li>
    <li><strong>Preprocessing:</strong> Apply border cropping and optional edge detection for robust feature matching</li>
    <li><strong>Alignment:</strong> Register the green and red channels to the blue reference using translation search</li>
    <li><strong>Reconstruction:</strong> Combine aligned channels into final RGB image with contrast enhancement</li>
</ol>

<p>
The alignment phase uses either single-scale exhaustive search for small images or multi-scale coarse-to-fine pyramid search for large images.
</p>

<hr>

<h2>Alignment Metrics</h2>
<p>Two primary similarity metrics were implemented and compared for channel registration:</p>

<div class="metric-section">
<h4>Sum of Squared Differences (SSD)</h4>
<p>The SSD measures pixel-wise squared differences between aligned image regions:</p>
<div class="formula">
<img src="https://latex.codecogs.com/svg.latex?\text{SSD}(\mathbf{I}_1,\mathbf{I}_2)=\sum_{x}\sum_{y}(\mathbf{I}_1(x,y)-\mathbf{I}_2(x,y))^2" alt="L2 Norm">
</div>
<p>Lower SSD values indicate better alignment. This metric is sensitive to intensity variations but provides smooth optimization landscapes for gradient-based refinement.</p>
</div>

<div class="metric-section">
<h4>Normalized Cross-Correlation (NCC)</h4>
<p>NCC normalizes images by their magnitudes, providing invariance to linear brightness and contrast changes:</p>
<div class="formula">
<img src="https://latex.codecogs.com/svg.latex?\text{NCC}(\mathbf{I}_1,\mathbf{I}_2)=\frac{\sum_{x}\sum_{y}\mathbf{I}_1(x,y)\mathbf{I}_2(x,y)}{\sqrt{\sum_{x}\sum_{y}\mathbf{I}_1(x,y)^2}\sqrt{\sum_{x}\sum_{y}\mathbf{I}_2(x,y)^2}}" alt="NCC">
</div>
<p>Higher NCC values indicate better alignment. This metric proved more robust for historical photographs with varying exposure conditions and lighting artifacts.</p>
</div>

<h3>Preprocessing Strategy</h3>
<p>
To improve alignment robustness, images undergo preprocessing before similarity computation. Edge detection using Sobel filters emphasizes structural features over smooth regions, while intensity normalization (zero mean, unit variance) reduces sensitivity to illumination variations.
</p>

<hr>

<h2>Part 1 — Single-Scale Alignment</h2>
<p>
For low-resolution images, exhaustive search over small displacement ranges (±15 pixels) is computationally feasible. 
The algorithm evaluates all possible integer translations and selects the offset that optimizes the chosen similarity metric.
Border cropping ensures alignment focuses on reliable central image content rather than potentially corrupted edges.
</p>

<div class="image-row">
    <figure>
        <img src="./result_cathedral.jpg" alt="Cathedral">
        <figcaption>Cathedral<br>G:(2, 5), R:(3, 12)</figcaption>
    </figure>
    <figure>
        <img src="./result_monastery.jpg" alt="Monastery">
        <figcaption>Monastery<br>G:(2, -3), R:(2, 3)</figcaption>
    </figure>
    <figure>
        <img src="./result_tobolsk.jpg" alt="Tobolsk">
        <figcaption>Tobolsk<br>G:(3, 3), R:(3, 6)</figcaption>
    </figure>
</div>

<hr>

<h2>Part 2 — Multi-Scale Pyramid Alignment</h2>
<p>
For high-resolution images, single-scale search becomes computationally prohibitive. The pyramid approach implements 
a coarse-to-fine strategy that dramatically reduces complexity while maintaining precision:
</p>

<ol>
    <li><strong>Pyramid Construction:</strong> Generate multi-resolution image hierarchies through iterative 2× downsampling</li>
    <li><strong>Coarse Alignment:</strong> Perform initial alignment at the smallest scale with large search windows</li>
    <li><strong>Progressive Refinement:</strong> Propagate and refine alignments at successively higher resolutions</li>
    <li><strong>Final Registration:</strong> Apply accumulated displacements to achieve sub-pixel precision</li>
</ol>

<div class="image-row">
    <figure><img src="./result_church.jpg" alt="Church"><figcaption>Church<br>G:(4, 25), R:(-4, 58)</figcaption></figure>
    <figure><img src="./result_emir.jpg" alt="Emir"><figcaption>Emir<br>G:(24, 49), R:(40, 107)</figcaption></figure>
    <figure><img src="./result_harvesters.jpg" alt="Harvesters"><figcaption>Harvesters<br>G:(17, 60), R:(14, 124)</figcaption></figure>
    <figure><img src="./result_icon.jpg" alt="Icon"><figcaption>Icon<br>G:(17, 42), R:(23, 90)</figcaption></figure>
    <figure><img src="./result_italil.jpg" alt="Italil"><figcaption>Italil<br>G:(22, 38), R:(36, 77)</figcaption></figure>
    <figure><img src="./result_lastochikino.jpg" alt="Lastochikino"><figcaption>Lastochikino<br>G:(-2, -3), R:(-8, 76)</figcaption></figure>
    <figure><img src="./result_lugano.jpg" alt="Lugano"><figcaption>Lugano<br>G:(-17, 41), R:(-29, 92)</figcaption></figure>
    <figure><img src="./result_melons.jpg" alt="Melons"><figcaption>Melons<br>G:(10, 80), R:(13, 177)</figcaption></figure>
    <figure><img src="./result_self_portrait.jpg" alt="Self Portrait"><figcaption>Self Portrait<br>G:(29, 78), R:(37, 176)</figcaption></figure>
    <figure><img src="./result_siren.jpg" alt="Siren"><figcaption>Siren<br>G:(-6, 49), R:(-24, 96)</figcaption></figure>
    <figure><img src="./result_three_generations.jpg" alt="Three Generations"><figcaption>Three Generations<br>G:(12, 54), R:(9, 111)</figcaption></figure>
</div>

<hr>

<h2>Personal Image Selection</h2>
<p>
To further validate the algorithm's robustness, four additional images were selected from the Prokudin-Gorskii collection 
based on their diverse photographic characteristics. These examples demonstrate the pipeline's performance across 
different scene types, lighting conditions, and structural complexities.
</p>

<div class="image-row">
    <figure>
        <img src="./r_candle.jpg" alt="Candle">
        <figcaption>Candle<br>G:(1, 48), R:(-7, 106)</figcaption>
    </figure>
    <figure>
        <img src="./r_cathedral.jpg" alt="Cathedral">
        <figcaption>Cathedral<br>G:(7, 44), R:(-2, 95)</figcaption>
    </figure>
    <figure>
        <img src="r_rocks.jpg" alt="Rocks">
        <figcaption>Rocks<br>G:(-16, 62), R:(-45, 117)</figcaption>
    </figure>
    <figure>
        <img src="r_beans.jpg" alt="Beans">
        <figcaption>Beans<br>G:(-38, 42), R:(-82, 108)</figcaption>
    </figure>
</div>

<hr>

<h2>Results and Discussion</h2>

<h3>Alignment Quality</h3>
<p>
The implemented pipeline successfully reconstructs high-quality color images from century-old glass plates. 
Displacement vectors show the extent of misalignment in the original captures, ranging from small corrections 
(2-3 pixels) for well-preserved plates to significant adjustments (>100 pixels) for damaged or shifted negatives.
</p>

<h3>Computational Efficiency</h3>
<p>
The multi-scale approach enables processing of high-resolution scans (3000+ pixels) in reasonable time. 
Automatic threshold-based selection between single-scale and pyramid methods ensures optimal performance 
across the full range of input image sizes.
</p>

<hr>

<h3>Bells & Whistles — Advanced Enhancements</h3>
<p>
Beyond basic channel alignment, several automatic enhancement techniques were implemented to improve 
image quality and robustness without manual intervention.
</p>

<h4>Automatic Cropping</h4>
<p>
The pipeline automatically removes border artifacts through a two-stage cropping strategy. During alignment, 
10% borders are cropped to focus similarity computation on reliable central content. After reconstruction, 
an additional 5% margin is removed from each edge to eliminate alignment artifacts and inconsistent border regions.
This approach effectively detects and removes problematic border areas that often contain color fringing or misaligned content.
</p>

<h4>Automatic Contrast Enhancement</h4>
<p>
Dynamic range optimization is performed by stretching image intensities to utilize the full [0,1] range. 
The algorithm identifies the darkest and brightest pixel values across all channels, then linearly rescales 
intensities such that the minimum becomes 0 and maximum becomes 1. This automatic contrast adjustment 
significantly improves visual quality, especially for historical photographs with limited dynamic range.
</p>

<h4>Edge-Based Feature Alignment</h4>
<p>
Instead of relying solely on raw pixel intensities, the alignment process can utilize Sobel edge detection 
to emphasize structural features over uniform regions. Edge preprocessing helps the algorithm focus on 
geometrically relevant features like building corners, facial contours, and object boundaries, rather than 
being misled by smooth areas with inconsistent lighting across channels.
</p>

<p>
<strong>Combined Impact:</strong> These enhancements work synergistically to produce higher quality reconstructions 
with improved color fidelity and better contrast  compared to basic channel alignment alone. 
</p>

<h3>Search Window Parameter Analysis</h3>
<p>
The choice of search window size (<code>max_shift_base</code>) impacts alignment quality, as demonstrated 
by the melons image. Contrary to intuition, larger search windows don't always produce better results:
</p>

<div class="image-row">
    <figure>
        <img src="result15_melons.jpg" alt="Melons with max_shift_base=15">
        <figcaption>Melons (max_shift_base=15)<br>Poor alignment - visible color fringing</figcaption>
    </figure>
    <figure>
        <img src="result_melons.jpg" alt="Melons with max_shift_base=50">
        <figcaption>Melons (max_shift_base=50)<br>Correct alignment - clean registration</figcaption>
    </figure>
</div>

</div>

</body>
</html>