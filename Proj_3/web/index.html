<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
    <title>Project 3: Image Mosaics and Homographies — CS180</title>
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']]
            }
        };
    </script>
<style>
    body {
        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
        margin: 0;
        padding: 40px;
        line-height: 1.7;
        background: linear-gradient(135deg, #f5f5f5 0%, #e0e0e0 100%);
        min-height: 100vh;
        color: #333;
    }
    
    .container {
        max-width: 1200px;
        margin: 0 auto;
        background: rgba(255, 255, 255, 0.95);
        border-radius: 20px;
        padding: 60px;
        box-shadow: 0 20px 60px rgba(0, 0, 0, 0.3);
        backdrop-filter: blur(10px);
    }
    
    header {
        display: flex;
        align-items: center;
        gap: 30px;
        margin-bottom: 50px;
        padding-bottom: 30px;
        border-bottom: 3px solid #666;
    }
    
    .header-text h1 { 
        font-size: 2.5rem; 
        margin: 0; 
        color: #2c3e50;
        font-weight: 700;
        background: linear-gradient(135deg, #666, #999);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
    }
    
    .header-text .subtitle {
        font-size: 1.1rem;
        color: #666;
        margin-top: 8px;
        font-weight: 500;
    }
    
    h2 { 
        margin-top: 50px; 
        margin-bottom: 25px; 
        color: #2c3e50; 
        font-size: 1.8rem;
        font-weight: 600;
        padding: 15px 25px;
        background: linear-gradient(135deg, #f9f9f9, #e9e9e9);
        border-radius: 12px;
        border-left: 5px solid #666;
    }
    
    h3 { 
        margin-top: 35px; 
        color: #444; 
        font-size: 1.3rem;
        font-weight: 600;
        border-bottom: 2px solid #eee;
        padding-bottom: 8px;
    }
    
    h4 {
        color: #555;
        font-size: 1.1rem;
        margin-top: 25px;
        font-weight: 600;
    }
    
    p { 
        color: #444; 
        font-size: 1.05rem;
        margin-bottom: 20px;
        text-align: justify;
    }
    
    ul, ol {
        color: #444;
        font-size: 1.05rem;
        line-height: 1.8;
    }
    
    li {
        margin-bottom: 8px;
    }
    
    .image-row { 
        display: flex; 
        gap: 30px; 
        flex-wrap: wrap; 
        justify-content: center; 
        margin: 40px 0; 
    }
    
    figure { 
        width: 280px; 
        text-align: center; 
        margin: 0;
        background: white;
        border-radius: 15px;
        padding: 20px;
        box-shadow: 0 8px 25px rgba(0, 0, 0, 0.15);
        transition: transform 0.3s ease, box-shadow 0.3s ease;
    }
    
    figure:hover {
        transform: translateY(-5px);
        box-shadow: 0 15px 35px rgba(0, 0, 0, 0.2);
    }
    
    figure img { 
        width: 100%; 
        height: auto; 
        border-radius: 10px;
        margin-bottom: 15px;
    }
    
    figcaption { 
        font-size: 0.95rem; 
        color: #666; 
        font-weight: 500;
        line-height: 1.4;
    }
    
    .full-width-figure {
        width: 100%;
        max-width: 1000px;
        margin: 40px auto;
        text-align: center;
        background: white;
        border-radius: 15px;
        padding: 30px;
        box-shadow: 0 8px 25px rgba(0, 0, 0, 0.15);
    }
    
    .full-width-figure img {
        width: 100%;
        height: auto;
        border-radius: 10px;
    }
    
    .full-width-figure figcaption {
        font-size: 1rem;
        color: #666;
        font-weight: 500;
        margin-top: 15px;
        line-height: 1.5;
    }
    
    .code-block {
        background: #2d2d2d;
        color: #f8f8f2;
        padding: 25px;
        border-radius: 12px;
        font-family: 'Courier New', monospace;
        font-size: 0.9rem;
        line-height: 1.6;
        overflow-x: auto;
        margin: 25px 0;
        box-shadow: 0 4px 15px rgba(0, 0, 0, 0.2);
    }
    
    .matrix-display {
        background: #f8f9fa;
        padding: 25px;
        border-radius: 12px;
        border-left: 5px solid #666;
        margin: 25px 0;
        overflow-x: auto;
        font-family: 'Courier New', monospace;
        font-size: 1rem;
        line-height: 1.8;
    }
    
    .highlight-box {
        background: #fff8e1;
        border-left: 5px solid #ff9800;
        padding: 20px;
        margin: 25px 0;
        border-radius: 8px;
        box-shadow: 0 2px 10px rgba(255, 152, 0, 0.1);
    }
    
    .info-box {
        background: #e3f2fd;
        border-left: 5px solid #2196f3;
        padding: 20px;
        margin: 25px 0;
        border-radius: 8px;
        box-shadow: 0 2px 10px rgba(33, 150, 243, 0.1);
    }
    
    .step-number {
        display: inline-block;
        background: #666;
        color: white;
        width: 40px;
        height: 40px;
        line-height: 40px;
        border-radius: 50%;
        text-align: center;
        font-weight: bold;
        margin-right: 15px;
        font-size: 1.1rem;
    }
    
    .comparison-grid {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 30px;
        margin: 40px 0;
    }
    
    @media (max-width: 768px) {
        .comparison-grid {
            grid-template-columns: 1fr;
        }
        .image-row {
            flex-direction: column;
            align-items: center;
        }
        figure {
            width: 100%;
            max-width: 400px;
        }
        .container {
            padding: 30px;
        }
    }
    
    .back-link {
        display: inline-block;
        background: linear-gradient(135deg, #666, #999);
        color: white;
        text-decoration: none;
        padding: 12px 25px;
        border-radius: 25px;
        font-weight: 600;
        margin-bottom: 30px;
        transition: all 0.3s ease;
        box-shadow: 0 4px 15px rgba(0, 0, 0, 0.2);
    }
    
    .back-link:hover {
        transform: translateY(-2px);
        box-shadow: 0 8px 25px rgba(0, 0, 0, 0.3);
    }
    
    table {
        width: 100%;
        border-collapse: collapse;
        margin: 25px 0;
        background: white;
        border-radius: 10px;
        overflow: hidden;
        box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1);
    }
    
    th, td {
        padding: 15px;
        text-align: left;
        border-bottom: 1px solid #eee;
    }
    
    th {
        background: #666;
        color: white;
        font-weight: 600;
    }
    
    tr:hover {
        background: #f8f9fa;
    }
    
    .footer {
        text-align: center;
        padding: 40px 0;
        color: #666;
        border-top: 2px solid #eee;
        margin-top: 60px;
        font-size: 0.95rem;
    }
</style>
</head>
<body>
    <div class="container">
        <a href="../../index.html" class="back-link">← Back to Portfolio</a>
        
        <header>
            <div class="header-text">
                <h1>Project 3: Image Mosaics and Homographies</h1>
                <div class="subtitle">Yuxuan Cai | UC Berkeley | Fall 2025</div>
            </div>
        </header>

        <!-- Introduction -->
        <h2>Overview</h2>
        <p>
            This project explores the mathematical foundations and practical implementation of image mosaicing through 
            homographic transformations. The primary objective is to seamlessly stitch multiple photographs with overlapping 
            fields of view into a unified panoramic image. This process requires a deep understanding of projective 
            geometry, robust correspondence point selection, homography computation, and sophisticated blending techniques 
            to eliminate visible seams.
        </p>
        <p>
            The project consists of two complementary parts: <strong>Part A</strong> implements manual mosaicing with 
            interactive correspondence point selection, while <strong>Part B</strong> develops a fully automatic 
            mosaicing pipeline using computer vision techniques. Part A provides the mathematical foundation through 
            manual homography estimation, while Part B demonstrates the transition to automatic feature detection, 
            matching, and robust statistical methods for homography estimation.
        </p>
        <p>
            The complete implementation showcases both manual and automatic approaches to image mosaicing, highlighting 
            the progression from human-guided to algorithmic computer vision techniques while maintaining high accuracy 
            and professional-quality results.
        </p>

        <!-- Part A: Manual Image Mosaicing -->
        <h2><span class="step-number">A</span>Manual Image Mosaicing</h2>
        
        <p>
            Part A establishes the mathematical foundation for image mosaicing through manual correspondence point 
            selection and homography computation. This approach provides complete control over the correspondence 
            process while demonstrating the core mathematical principles underlying projective transformations.
        </p>

        <!-- A.1: Shoot and Digitize Pictures -->
        <h3><span class="step-number">A.1</span>Shoot and Digitize Pictures</h3>
        
        <p>
            I captured multiple sets of photographs with projective transformations between them by fixing the center 
            of projection and rotating the camera while capturing photos. Each set has 40-70% overlap as recommended, 
            with images taken close together in time to minimize subject movement and lighting changes.
        </p>

        <h3>Image Set 1: Sproul Hall Panorama</h3>
        <p>
            Two overlapping views of Sproul Hall on the UC Berkeley campus, captured with approximately 50% overlap. 
            The images were taken from slightly different positions with the camera rotated around a fixed center of projection.
        </p>
        
        <div class="image-row">
            <figure>
                <img src="../images/sproul_left.jpeg" alt="Sproul Left">
                <figcaption>Sproul Hall - Left View<br>
                <small>Resolution: 4032 × 3024 pixels</small></figcaption>
            </figure>
            <figure>
                <img src="../images/sproul_right.jpeg" alt="Sproul Right">
                <figcaption>Sproul Hall - Right View<br>
                <small>Resolution: 4032 × 3024 pixels</small></figcaption>
            </figure>
        </div>

        <h3>Image Set 2: Wheeler Hall Panorama</h3>
        <p>
            Two overlapping views of Wheeler Hall, another prominent building on campus. This set demonstrates 
            the pipeline's robustness across different architectural scenes and lighting conditions.
        </p>
        
        <div class="image-row">
            <figure>
                <img src="../images/wheeler_left.jpeg" alt="Wheeler Left">
                <figcaption>Wheeler Hall - Left View<br>
                <small>Resolution: 4032 × 3024 pixels</small></figcaption>
            </figure>
            <figure>
                <img src="../images/wheeler_right.jpeg" alt="Wheeler Right">
                <figcaption>Wheeler Hall - Right View<br>
                <small>Resolution: 4032 × 3024 pixels</small></figcaption>
            </figure>
        </div>

        <!-- A.2: Recover Homographies -->
        <h3><span class="step-number">A.2</span>Recover Homographies</h3>
        
        <h3>Manual Correspondence Point Selection</h3>
        <p>
            The foundation of accurate image alignment lies in identifying corresponding feature points across images 
            with overlapping content. I developed an interactive point selection tool that enables precise manual 
            identification of matching features between image pairs. The tool displays images sequentially and captures 
            mouse click coordinates, ensuring that points are selected in corresponding order.
        </p>

        <h4>Point Selection Strategy and Quality Criteria</h4>
        <p>
            Through systematic experimentation, I discovered how different point selection strategies affect homography quality:
        </p>

        <ul>
            <li><strong>Feature type:</strong> Corner features and high-contrast edges provide more reliable correspondences than low-contrast or poorly-defined features. Architectural corners, window frames, and building edges are ideal.</li>
            <li><strong>Spatial distribution:</strong> Points should be well-distributed across the overlap region to provide robust geometric constraints. Clustered points lead to poor homography estimation.</li>
            <li><strong>Point count:</strong> 6 correspondence pairs provide sufficient constraints (12 equations for 8 unknowns) while maintaining robustness against minor selection errors. More points improve accuracy but increase selection time.</li>
            <li><strong>Selection precision:</strong> Small errors (±2-3 pixels) in point selection propagate through the homography computation but are mitigated by using overdetermined systems.</li>
        </ul>

        <div class="highlight-box">
            <strong>Key Learning:</strong> The quality and spatial distribution of correspondence points critically 
            determine the accuracy of the computed homography. Points should be well-distributed across the overlap 
            region and selected on high-contrast, geometrically stable features such as architectural corners, window 
            frames, and building edges. I used 6 correspondence pairs to provide sufficient constraints while maintaining robustness against minor selection errors.
        </div>

        <h3>Correspondence Visualization</h3>
        <p>
            I manually selected 6 pairs of corresponding points for each image set, focusing on distinctive architectural 
            features visible in both images. The points were chosen to span the entire overlap region to ensure robust 
            homography estimation.
        </p>

        <h4>Sproul Hall Correspondences</h4>
        <div class="image-row">
            <figure>
                <img src="../results/correspondence_visualization.jpg" alt="Sproul Correspondences">
                <figcaption>Sproul Hall - Point Correspondences<br>
                <small>6 pairs of corresponding points selected</small></figcaption>
            </figure>
        </div>

        <h4>Wheeler Hall Correspondences</h4>
        <div class="image-row">
            <figure>
                <img src="../results/wheeler_correspondences.jpg" alt="Wheeler Correspondences">
                <figcaption>Wheeler Hall - Point Correspondences<br>
                <small>6 pairs of corresponding points selected</small></figcaption>
            </figure>
        </div>

        
        <div class="info-box">
            <strong>Point Selection Strategy:</strong> I prioritized corner features and high-contrast edges that 
            are easily identifiable in both images. The selected points include building corners, window edges, 
            and architectural details that exhibit minimal motion parallax.
        </div>

        <h3>Direct Linear Transform (DLT) for Homography Estimation</h3>
        <p>
            The homography estimation problem involves finding a 3×3 transformation matrix $H$ that maps points from 
            one image plane to another. For each correspondence pair $(x, y) \leftrightarrow (x', y')$, we set up 
            two linear equations by cross-multiplying the projective equations. With 6 point pairs, we get 12 
            equations for 8 unknowns (after normalization), creating an overdetermined system.
        </p>

        <h4>Mathematical Foundation</h4>
        <p>
            A homography $H$ maps points from one image to another through the projective transformation:
        </p>
        <p>$$\begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} = \begin{bmatrix} h_{11} & h_{12} & h_{13} \\ h_{21} & h_{22} & h_{23} \\ h_{31} & h_{32} & h_{33} \end{bmatrix} \begin{bmatrix} x \\ y \\ 1 \end{bmatrix}$$</p>
        
        <p>Cross-multiplying gives us the constraint equations:</p>
        <p>$$x' = \frac{h_{11}x + h_{12}y + h_{13}}{h_{31}x + h_{32}y + h_{33}}$$</p>
        <p>$$y' = \frac{h_{21}x + h_{22}y + h_{23}}{h_{31}x + h_{32}y + h_{33}}$$</p>

        <div class="matrix-display">
            <p><strong>System of Equations for Homography Estimation:</strong></p>
            <p>For each correspondence pair $(x, y) \leftrightarrow (x', y')$:</p>
            
            <p><strong>Row 1:</strong> $[x \quad y \quad 1 \quad 0 \quad 0 \quad 0 \quad -x'x \quad -x'y \quad -x'] \cdot \mathbf{h} = 0$</p>
            <p><strong>Row 2:</strong> $[0 \quad 0 \quad 0 \quad x \quad y \quad 1 \quad -y'x \quad -y'y \quad -y'] \cdot \mathbf{h} = 0$</p>
            
            <p>Where $\mathbf{h} = [h_{11}, h_{12}, h_{13}, h_{21}, h_{22}, h_{23}, h_{31}, h_{32}, h_{33}]^T$</p>
            
            <p>This creates the overdetermined system: $\mathbf{A}\mathbf{h} = \mathbf{0}$</p>
            
            <p><strong>Solution using SVD:</strong></p>
            <p>$\mathbf{h}$ is the right singular vector corresponding to the smallest singular value of $\mathbf{A}$</p>
        </div>

        <h3>Computed Homography Matrices</h3>
        
        <h4>Sproul Hall Homography</h4>
        <div class="matrix-display">
            <pre>H_sproul = [[ 1.7323   -0.0421  -2159.23]
           [ 0.3341    1.5110   -867.07]
           [ 0.00018   0.00002    1.0000]]</pre>
        </div>

        <h4>Wheeler Hall Homography</h4>
        <div class="matrix-display">
            <pre>H_wheeler = [[ 2.2078   -0.0341  -3316.20]
            [ 0.5454    1.9036  -1530.97]
            [ 0.00029   0.00004    1.0000]]</pre>
        </div>

        <h4>House Homography</h4>
        <div class="matrix-display">
            <pre>H_house = [[-0.3498   -0.1735    955.21]
          [-0.4946   -0.2158   1339.83]
          [-0.00036  -0.00017     1.0000]]</pre>
        </div>

        <div class="info-box">
            <strong>Matrix Interpretation:</strong> All homographies show similar characteristics - diagonal scaling 
            elements, small rotation/shear terms, and large translation components. The perspective terms are small 
            but non-zero, capturing subtle projective distortions from the camera rotation around a fixed center of projection.
            The house homography shows negative scaling values, indicating a different camera orientation or scene geometry.
        </div>

        <!-- A.3: Warp the Images -->
        <h3><span class="step-number">A.3</span>Warp the Images</h3>
        
        <h3>Image Warping with Inverse Mapping</h3>
        <p>
            I implemented both required interpolation methods from scratch using inverse warping to avoid holes 
            in the output image. Inverse warping guarantees that every output pixel receives a value by computing 
            the corresponding location in the source image using $H^{-1}$.
        </p>

        <h4>Inverse Warping Process</h4>
        <p>
            For each output pixel $(x_{out}, y_{out})$:
        </p>
        <p>$$\begin{bmatrix} x_{src} \\ y_{src} \\ 1 \end{bmatrix} = H^{-1} \begin{bmatrix} x_{out} \\ y_{out} \\ 1 \end{bmatrix}$$</p>
        <p>Then sample the source image at $(x_{src}, y_{src})$ using interpolation.</p>

        <h4>1. Nearest Neighbor Interpolation</h4>
        <p>
            For each output pixel, round the source coordinates to the nearest integer pixel and sample directly:
        </p>
        <p>$$I_{out}(x_{out}, y_{out}) = I_{src}(\lfloor x_{src} + 0.5 \rfloor, \lfloor y_{src} + 0.5 \rfloor)$$</p>
        <p>This method is fast but produces blocky, aliased results.</p>

        <h4>2. Bilinear Interpolation</h4>
        <p>
            For each output pixel, interpolate from the four nearest integer neighbors using distance-weighted averaging:
        </p>

        <div class="matrix-display">
            <p><strong>Bilinear Interpolation Formula:</strong></p>
            <p>Let: $x_1 = \lfloor x \rfloor$, $x_2 = x_1 + 1$, $y_1 = \lfloor y \rfloor$, $y_2 = y_1 + 1$</p>
            <p>$w_x = x - x_1$ (horizontal weight)</p>
            <p>$w_y = y - y_1$ (vertical weight)</p>
            
            <p><strong>Interpolated Value:</strong></p>
            <p>$$I(x,y) = (1-w_x)(1-w_y) \cdot I(x_1,y_1) + w_x(1-w_y) \cdot I(x_2,y_1) + (1-w_x)w_y \cdot I(x_1,y_2) + w_x w_y \cdot I(x_2,y_2)$$</p>
            
            <p>This provides smooth interpolation by weighting the four nearest integer neighbors based on their distance to the target point.</p>
        </div>

        <h3>Comparison of Interpolation Methods</h3>
        <div class="image-row">
            <figure>
                <img src="../results/warped_nn_sproul.jpg" alt="Nearest Neighbor">
                <figcaption>Nearest Neighbor Interpolation<br>
                </figcaption>
            </figure>
            <figure>
                <img src="../results/warped_bil_sproul.jpg" alt="Bilinear">
                <figcaption>Bilinear Interpolation<br>
                </figcaption>
            </figure>
        </div>

        <div class="highlight-box">
            <strong>Trade-offs:</strong> Nearest neighbor is ~3x faster but produces visible pixelation artifacts. 
            Bilinear interpolation provides smooth, professional-quality results at the cost of computational overhead. 
            For mosaicing applications, bilinear interpolation is essential for seamless blending.
        </div>

        <h3>Rectification Examples</h3>
        <p>
            To verify the homography and warping implementation, I performed rectification on images containing 
            rectangular objects. Rectification transforms tilted rectangular features into perfect squares, 
            demonstrating the accuracy of the projective transformation pipeline.
        </p>

        <h4>Rectification Example 1: House - Window Frame</h4>

        
        <div class="full-width-figure">
            <img src="../results/rectification_house1_comparison.jpg" alt="Rectification Comparison 1">
            <figcaption>Complete Rectification Process - House Left<br>
            <small>Before and after comparison showing the rectification transformation</small></figcaption>
        </div>

        <h4>Rectification Example 2: House - Another Window frame</h4>

        
        <div class="full-width-figure">
            <img src="../results/rectification_house2_comparison.jpg" alt="Rectification Comparison 2">
            <figcaption>Complete Rectification Process - House Right<br>
            <small>Before and after comparison showing the rectification transformation</small></figcaption>
        </div>

        <div class="info-box">
            <strong>Rectification Success:</strong> Both rectification examples demonstrate that the homography 
            computation and warping implementation are working correctly. The rectangular features are successfully 
            transformed into perfect squares, confirming the accuracy of the projective transformation pipeline. 
            This validates the mathematical foundation and implementation of the homography estimation and image warping algorithms.
        </div>

        <!-- A.4: Blend Images into Mosaic -->
        <h3><span class="step-number">A.4</span>Blend Images into a Mosaic</h3>
        
        <h3>Global Canvas Strategy and Coordinate System Design</h3>
        <p>
            Creating a seamless mosaic requires establishing a global coordinate system that accommodates both 
            images after transformation. I compute the global canvas by projecting all four corners of each 
            image through their respective homographies and determining the minimum bounding box. This ensures 
            the canvas is large enough to contain both transformed images without clipping.
        </p>

        <h4>Canvas Design Considerations</h4>
        <p>
            Through experimentation, I discovered how different canvas strategies affect mosaic quality:
        </p>

        <ul>
            <li><strong>Bounding box approach:</strong> Projecting all corners and finding global bounds ensures no clipping but may create large empty regions. This is necessary for arbitrary homographies.</li>
            <li><strong>Coordinate system choice:</strong> Using the first image as reference (identity transformation) simplifies computation but may not be optimal for all cases.</li>
            <li><strong>Memory efficiency:</strong> Large canvases increase memory usage but provide flexibility for complex transformations.</li>
            <li><strong>Positioning strategy:</strong> Placing images with positive coordinates avoids negative indexing issues and simplifies blending operations.</li>
        </ul>

        <div class="matrix-display">
            <p><strong>Global Canvas Computation:</strong></p>
            <ol>
                <li><strong>Transform all corners:</strong> $\mathbf{c}'_i = H \cdot \mathbf{c}_i$ for each corner</li>
                <li><strong>Find global bounds:</strong>
                    <ul>
                        <li>$x_{\min} = \min(\text{all } x')$, $x_{\max} = \max(\text{all } x')$</li>
                        <li>$y_{\min} = \min(\text{all } y')$, $y_{\max} = \max(\text{all } y')$</li>
                    </ul>
                </li>
                <li><strong>Canvas size:</strong>
                    <ul>
                        <li>$\text{width} = x_{\max} - x_{\min}$</li>
                        <li>$\text{height} = y_{\max} - y_{\min}$</li>
                    </ul>
                </li>
            </ol>
            
            <p>This ensures the canvas is large enough to contain both transformed images without clipping.</p>
        </div>

        <h3>Alpha Feathering for Seamless Blending</h3>
        <p>
            Simple pixel averaging in overlap regions often produces visible seams due to exposure variations, 
            vignetting, and residual alignment errors. I implemented <strong>alpha feathering</strong>, where 
            each pixel's contribution is weighted by its distance from image boundaries. The alpha map transitions 
            smoothly from 1.0 at the image center to 0.0 at the edges.
        </p>

        <h4>Alpha Feathering Parameter Tuning</h4>
        <p>
            Through systematic experimentation, I discovered how different alpha feathering parameters affect blending quality:
        </p>

        <ul>
            <li><strong>Feather ratio (0.2):</strong> Controls the transition zone width. Lower values (0.1-0.15) create sharper transitions but may show seams. Higher values (0.3-0.4) create smoother blends but may blur details.</li>
            <li><strong>Distance metric:</strong> Using minimum distance to any edge provides uniform feathering. Alternative metrics like distance to nearest edge can create different effects.</li>
            <li><strong>Clipping strategy:</strong> Clipping alpha values to [0,1] prevents overshoot but may create hard edges. Smooth falloff functions provide more natural transitions.</li>
            <li><strong>Normalization:</strong> Normalizing by the shorter image dimension ensures consistent feathering across different image sizes.</li>
        </ul>

        <div class="matrix-display">
            <p><strong>Alpha Feathering Algorithm:</strong></p>
            <p>For each pixel $(x, y)$:</p>
            <p>$$\text{distance\_to\_edge} = \min(x, y, \text{width}-x, \text{height}-y)$$</p>
            <p>$$\alpha(x,y) = \text{clip}\left(\frac{\text{distance\_to\_edge}}{\text{feather\_ratio} \times \min(\text{width}, \text{height})/2}, 0, 1\right)$$</p>
            
            <p><strong>Final pixel value:</strong></p>
            <p>$$I_{\text{mosaic}} = \frac{I_1 \cdot \alpha_1 + I_2 \cdot \alpha_2}{\alpha_1 + \alpha_2}$$</p>
            
            <p>Where $\text{feather\_ratio} = 0.2$ creates smooth transitions spanning ~20% of the shorter image dimension.</p>
        </div>

        <p>
            I used a feather_ratio of 0.2, which creates a smooth transition zone spanning approximately 20% 
            of the shorter image dimension from each edge. This produces natural-looking blends without visible 
            seam artifacts.
        </p>

        <div class="highlight-box">
            <strong>Key Learning:</strong> Alpha feathering is superior to hard-edged blending or simple averaging 
            because it gradually transitions between images in the overlap region. Pixels near image centers 
            (high confidence regions) receive higher weights, while edge pixels (lower confidence due to distortion 
            and vignetting) receive lower weights. This creates seamless transitions even with slight exposure or 
            alignment variations.
        </div>

        <h3>Intermediate Processing Steps</h3>
        <p>
            The following visualizations show how each image is positioned on the global canvas before blending:
        </p>
        
        <div class="image-row">
            <figure>
                <img src="../results/canvas1_sproul_fixed.jpg" alt="Canvas 1">
                <figcaption>Image 1 placed on global canvas</figcaption>
            </figure>
            <figure>
                <img src="../results/canvas2_sproul_fixed.jpg" alt="Canvas 2">
                <figcaption>Image 2 warped and placed on global canvas</figcaption>
            </figure>
        </div>

        <h3>Alpha Map Visualization</h3>
        <p>
            The alpha blending weights show how each image contributes to the final mosaic. Red represents 
            Image 2's alpha map, green represents Image 1's alpha map, and yellow indicates overlap regions 
            where both images contribute:
        </p>
        <h3>Complete Mosaics</h3>
        <p>
            I created two different mosaics to demonstrate the pipeline's versatility across different scenes 
            and lighting conditions. Each mosaic uses weighted averaging with alpha feathering to eliminate edge artifacts.
        </p>

        <h4>Mosaic 1: Sproul Hall Panorama</h4>
        <div class="full-width-figure">
            <img src="../results/mosaic_sproul_fixed.jpg" alt="Sproul Mosaic">
            <figcaption>Complete Panoramic Mosaic of Sproul Hall<br>
            <small>Final resolution: 6574 × 4401 pixels </small></figcaption>
        </div>

        <h4>Mosaic 2: Wheeler Hall Panorama</h4>
        <div class="full-width-figure">
            <img src="../results/mosaic_wheeler_fixed.jpg" alt="Wheeler Mosaic">
            <figcaption>Complete Panoramic Mosaic of Wheeler Hall<br>
            <small>Final resolution: 5301 × 4381 pixels</small></figcaption>
        </div>
        
        <div class="info-box">
            <strong>Note on Wheeler Mosaic:</strong> The Wheeler Hall mosaic shows some blending artifacts in areas where 
            students were moving during the image capture process. This demonstrates a limitation of the current 
            approach when dealing with dynamic scenes - the homography assumes a static scene, so moving objects 
            in the overlap region can create ghosting or double-exposure effects in the final mosaic.
        </div>

        <h4>Mosaic 3: House Panorama</h4>
        <div class="full-width-figure">
            <img src="../results/mosaic_house_fixed.jpg" alt="House Mosaic">
            <figcaption>Complete Panoramic Mosaic of House<br>
            <small>Final resolution: 4032 × 3024 pixels</small></figcaption>
        </div>
        
        <div class="info-box">
            <strong>House Mosaic Features:</strong> This residential house mosaic demonstrates the pipeline's 
            versatility across different architectural styles and lighting conditions. The house images present 
            unique challenges with varied textures, different building materials, and complex geometric features 
            that test the robustness of the homography computation and blending algorithms.
        </div>


        <div class="highlight-box">
            <strong>Blending Procedure:</strong> All mosaics use the same sophisticated blending approach: (1) Global 
            canvas prediction by projecting all image corners through homographies, (2) Alpha feathering with 
            distance-based weights that fall off from image centers to edges, (3) Weighted averaging in overlap 
            regions to eliminate visible seams, and (4) Gaussian blur for smoother transitions. This procedure 
            produces professional-quality results that rival commercial panorama software.
        </div>



        <!-- Part B: Automatic Image Mosaicing -->
        <h2><span class="step-number">B</span>Automatic Image Mosaicing</h2>

        <!-- B.1: Harris Corner Detection with ANMS -->
        <h3><span class="step-number">B.1</span>Harris Corner Detection with ANMS</h3>
        
        <p>
            The first step in automatic mosaicing is detecting distinctive corner features that can be reliably 
            matched between images. I implemented the Harris corner detector, which identifies points where the 
            image intensity changes significantly in all directions. The algorithm computes a corner response 
            function R that measures the "cornerness" of each pixel.
        </p>

        <h4>Mathematical Foundation</h4>
        <p>
            The Harris corner detector is based on the second moment matrix M, which captures the local image 
            structure around each pixel:
        </p>

        <div class="matrix-display">
            <p>The Harris corner detector is based on the second moment matrix <strong>M</strong>:</p>
            <p>$$M = \sum w(x,y) \begin{bmatrix} I_x^2 & I_x I_y \\ I_x I_y & I_y^2 \end{bmatrix}$$</p>
            
            <p>Where:</p>
            <ul>
                <li>$I_x, I_y$ are image gradients in x and y directions (Sobel gradients)</li>
                <li>$w(x,y)$ is a Gaussian weighting function</li>
                <li>The sum is over a local window around each pixel</li>
            </ul>
            
            <p><strong>Corner Response:</strong></p>
            <p>$$R = \det(M) - k \cdot \text{trace}(M)^2$$</p>
            <p>Where $k \approx 0.04-0.06$ (typically 0.04)</p>
        </div>

        <p>
            The corner response R is large when both eigenvalues of M are large, indicating significant 
            intensity changes in all directions. This makes corners ideal for matching because they are 
            distinctive and relatively invariant to small viewpoint changes.
        </p>

        <h4>Parameter Tuning and Effects</h4>
        <p>
            Through extensive experimentation, I discovered how different parameters affect corner detection quality:
        </p>

        <ul>
            <li><strong>Harris k value (0.04):</strong> Controls sensitivity to corner strength. Lower values (0.02-0.03) detect more corners but include more noise. Higher values (0.05-0.08) are more selective but may miss weaker corners.</li>
            <li><strong>Threshold (0.01):</strong> Minimum corner response for detection. Lower thresholds find more corners but increase false positives. Higher thresholds are more selective but may miss important features.</li>
            <li><strong>Gaussian sigma (1.0):</strong> Controls smoothing of gradients. Larger values (1.5-2.0) reduce noise but blur fine details. Smaller values (0.5-0.8) preserve details but are more sensitive to noise.</li>
            <li><strong>Target corners (500):</strong> Number of corners after ANMS. More corners provide better coverage but increase computational cost. Fewer corners are faster but may miss important regions.</li>
        </ul>

        <h4>Adaptive Non-Maximal Suppression (ANMS)</h4>
        <p>
            Raw Harris corner detection often produces clusters of corners in high-contrast regions, 
            which can lead to poor feature distribution. ANMS addresses this by selecting corners that 
            are both strong and well-distributed across the image.
        </p>

        <div class="matrix-display">
            <pre>ANMS Algorithm:
1. Sort corners by response strength (descending)
2. For each corner, find minimum distance to stronger corners
3. Select corners with largest minimum distances
4. Continue until target number of corners is reached

This ensures corners are both distinctive and spatially distributed.</pre>
        </div>

        <h4>Before vs After ANMS Results</h4>
        <p>
            The following comparisons demonstrate the dramatic improvement in corner distribution achieved by ANMS:
        </p>

        <div class="full-width-figure">
            <img src="../resultsB/before_after_ANMS_house.jpg" alt="House ANMS Comparison">
            <figcaption>House Building - Before vs After ANMS<br>
            <small>Left: Clustered corners. Right: Well-distributed corners after ANMS</small></figcaption>
        </div>

        <div class="full-width-figure">
            <img src="../resultsB/before_after_ANMS_sproul.jpg" alt="Sproul ANMS Comparison">
            <figcaption>Sproul Hall - Before vs After ANMS<br>
            <small>ANMS ensures spatial coverage across the entire image</small></figcaption>
        </div>

        <div class="full-width-figure">
            <img src="../resultsB/before_after_ANMS_wheeler.jpg" alt="Wheeler ANMS Comparison">
            <figcaption>Wheeler Hall - Before vs After ANMS<br>
            <small>Balanced distribution of distinctive architectural features</small></figcaption>
        </div>

        <div class="highlight-box">
            <strong>Key Learning:</strong> ANMSimproves feature distribution compared to simple 
            thresholding. 
            This spatial distribution is crucial for robust homography estimation. Without ANMS, corners cluster 
            in high-contrast regions, leading to poor feature matching and unreliable homography estimation.
        </div>

        <!-- B.2: Feature Descriptor Extraction -->
        <h3><span class="step-number">B.2</span>Feature Descriptor Extraction</h3>
        
        <p>
            Once distinctive corners are identified, the next step is to create robust descriptors that 
            can be reliably matched between images. I implemented the approach from Brown et al., extracting 
            8×8 normalized patches from 40×40 windows around each corner point.
        </p>

        <h4>Descriptor Extraction Process</h4>
        <p>
            For each detected corner, I extract a 40×40 window and sample an 8×8 patch from its center 
            using bilinear interpolation. The patch is then normalized to be invariant to illumination changes:
        </p>

        <div class="matrix-display">
            <p><strong>Descriptor Extraction Process:</strong></p>
            <ol>
                <li>Extract 40×40 window around corner $(x, y)$</li>
                <li>Sample 8×8 patch from center using bilinear interpolation</li>
                <li>Apply bias/gain normalization:</li>
            </ol>
            
            <p><strong>Normalization Formulas:</strong></p>
            <p>$$\mu = \frac{1}{N} \sum_{i=1}^{N} p_i$$</p>
            <p>$$\sigma = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (p_i - \mu)^2}$$</p>
            <p>$$\text{normalized}_i = \frac{p_i - \mu}{\sigma}$$</p>
            
            <p>Where $N = 64$ (8×8 patch), $p_i$ are pixel values</p>
            
            <p><strong>This creates 64-dimensional descriptors that are:</strong></p>
            <ul>
                <li><strong>Illumination invariant</strong> (bias normalization)</li>
                <li><strong>Scale invariant</strong> (gain normalization)</li>
                <li><strong>Rotation invariant</strong> (axis-aligned patches)</li>
            </ul>
        </div>

        <h4>Parameter Effects on Descriptor Quality</h4>
        <p>
            Through experimentation, I discovered how different parameters affect descriptor quality and matching performance:
        </p>

        <ul>
            <li><strong>Window size (40×40):</strong> Larger windows capture more context but are more sensitive to viewpoint changes. Smaller windows (32×32) are more robust to rotation but may lack distinctive features.</li>
            <li><strong>Patch size (8×8):</strong> Larger patches (12×12) contain more information but are more sensitive to geometric distortions. Smaller patches (6×6) are more robust but may lack distinctiveness.</li>
            <li><strong>Bilinear interpolation:</strong> Essential for sub-pixel accuracy. Without interpolation, descriptors become aliased and unreliable for matching.</li>
            <li><strong>Normalization:</strong> Critical for illumination invariance. Without normalization, descriptors vary significantly with lighting changes, leading to poor matching.</li>
        </ul>

        <h4>Feature Descriptor Results</h4>
        <p>
            The following visualizations show the extracted feature descriptors for each image pair, demonstrating 
            the distinctive patterns that enable reliable matching:
        </p>

        <div class="full-width-figure">
            <img src="../resultsB/feature_house.jpg" alt="House Feature Descriptors">
            <figcaption>House Building - Feature Descriptors<br>
            <small>8×8 normalized patches showing distinctive architectural patterns</small></figcaption>
        </div>

        <div class="full-width-figure">
            <img src="../resultsB/feature_sproul.jpg" alt="Sproul Feature Descriptors">
            <figcaption>Sproul Hall - Feature Descriptors<br>
            <small>Well-distributed descriptors across the image</small></figcaption>
        </div>

        <div class="full-width-figure">
            <img src="../resultsB/feature_wheeler.jpg" alt="Wheeler Feature Descriptors">
            <figcaption>Wheeler Hall - Feature Descriptors<br>
            <small>High-quality descriptors for robust matching</small></figcaption>
        </div>

        <div class="info-box">
            <strong>Descriptor Properties:</strong> The extracted descriptors show patterns corresponding 
            to architectural features like window frames, building edges, and texture details. The normalization 
            ensures that descriptors remain consistent even when lighting conditions change between images, 
            making them robust for automatic matching. 
        </div>

        <!-- B.3: Feature Matching with Ratio Test -->
        <h3><span class="step-number">B.3</span>Feature Matching with Ratio Test</h3>
        
        <p>
            Matching features between images requires finding the most similar descriptors while rejecting 
            ambiguous matches. I implemented Lowe's ratio test, which compares the distance to the nearest 
            neighbor with the distance to the second-nearest neighbor.
        </p>

        <h4>Matching Algorithm</h4>
        <p>
            For each descriptor in image 1, I find the two nearest neighbors in image 2 using Euclidean distance. 
            The ratio test accepts a match only if the first distance is significantly smaller than the second:
        </p>

        <div class="matrix-display">
            <p><strong>Matching Process:</strong></p>
            <ol>
                <li>For each descriptor $d_1$ in image 1:
                    <ul>
                        <li>Find nearest neighbor $d_{2,1}$ in image 2</li>
                        <li>Find second nearest neighbor $d_{2,2}$ in image 2</li>
                        <li>Compute ratio: $r = \frac{||d_1 - d_{2,1}||}{||d_1 - d_{2,2}||}$</li>
                        <li>Accept match if $r < \text{threshold}$ (typically 0.8)</li>
                    </ul>
                </li>
                <li>Apply bidirectional matching:
                    <ul>
                        <li>Match image 1 → image 2</li>
                        <li>Match image 2 → image 1</li>
                        <li>Keep only mutual matches</li>
                    </ul>
                </li>
            </ol>
            
            <p><strong>Distance Calculation:</strong></p>
            <p>$$||d_1 - d_2|| = \sqrt{\sum_{i=1}^{64} (d_{1,i} - d_{2,i})^2}$$</p>
            
            <p>This eliminates many false matches and improves robustness.</p>
        </div>

        <h4>Parameter Tuning and Quality Assessment</h4>
        <p>
            Through extensive experimentation, I discovered how the ratio threshold affects matching quality:
        </p>

        <ul>
            <li><strong>Ratio threshold (0.8):</strong> Lower values (0.6-0.7) are more selective but may reject valid matches. Higher values (0.9-1.0) accept more matches but include more false positives. 0.8 provides the best balance.</li>
            <li><strong>Mean distance:</strong> Lower mean distance indicates better match quality. Values below 3.0 are excellent, 3.0-5.0 are good, 5.0-8.0 are fair, and above 8.0 are poor.</li>
            <li><strong>Bidirectional matching:</strong> Essential for robustness. Unidirectional matching can create asymmetric correspondences that lead to poor homography estimation.</li>
            <li><strong>Match count:</strong> Need at least 10+ matches for reliable homography estimation. Too few matches lead to unstable homographies, while too many may include more outliers.</li>
        </ul>

        <h4>Feature Matching Results</h4>
        <p>
            The following visualizations show the feature matching results for each image pair, demonstrating 
            the effectiveness of the ratio test in identifying high-quality correspondences:
        </p>

        <div class="full-width-figure">
            <img src="../resultsB/house_match.jpeg" alt="House Feature Matches">
            <figcaption>House Building - Feature Matches<br>
            <small>Bidirectional matching with ratio test filtering</small></figcaption>
        </div>

        <div class="full-width-figure">
            <img src="../resultsB/sproul_match.jpeg" alt="Sproul Feature Matches">
            <figcaption>Sproul Hall - Feature Matches<br>
            <small>Bidirectional matching with ratio test filtering</small></figcaption>
        </div>

        <div class="full-width-figure">
            <img src="../resultsB/wheeler_match.jpeg" alt="Wheeler Feature Matches">
            <figcaption>Wheeler Hall - Feature Matches<br>
            <small>Bidirectional matching with ratio test filtering</small></figcaption>
        </div>

        <!-- B.4: RANSAC Homography Estimation -->
        <h3><span class="step-number">B.4</span>RANSAC Homography Estimation</h3>
        
        <p>
            Even with careful feature matching, some correspondences will be incorrect due to repetitive 
            patterns, occlusions, or similar-looking features. RANSAC (Random Sample Consensus) provides 
            a robust method to estimate the homography while automatically identifying and rejecting outliers.
        </p>

        <h4>Automatic Homography Estimation Pipeline</h4>
        <p>
            From the B.3 feature matches $(P_i \leftrightarrow Q_i)$, I implement a complete automatic homography 
            estimation pipeline using 4-point RANSAC. This robust approach automatically handles outliers and 
            produces reliable homographies for seamless image mosaicing.
        </p>

        <h5>4-Point RANSAC Implementation</h5>
        <p>
            The RANSAC algorithm iteratively estimates homographies by randomly sampling minimal point sets:
        </p>

        <div class="matrix-display">
            <p><strong>RANSAC Process:</strong></p>
            <ol>
                <li><strong>Random Sampling:</strong> Each iteration randomly samples 4 correspondences $(P_i, Q_i)$</li>
                <li><strong>Degeneracy Check:</strong> Skip degenerate quads with near-zero area to ensure numerical stability</li>
                <li><strong>Homography Fitting:</strong> Compute $H$ using both normalized DLT and $Ah=b$ methods, keeping the best result</li>
                <li><strong>Inlier Scoring:</strong> Evaluate all matches using reprojection error: $||HP-Q||_2$</li>
                <li><strong>Threshold Test:</strong> Count inliers where error $< 3$ pixels (configurable threshold)</li>
                <li><strong>Model Selection:</strong> Keep the homography with the largest inlier count</li>
                <li><strong>Final Estimation:</strong> Re-estimate $H$ on all inliers using the chosen estimator</li>
            </ol>
            
            <p><strong>Mathematical Foundation:</strong></p>
            <p><strong>Homography Transformation:</strong></p>
            <p>$$\begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} = \begin{bmatrix} h_{11} & h_{12} & h_{13} \\ h_{21} & h_{22} & h_{23} \\ h_{31} & h_{32} & h_{33} \end{bmatrix} \begin{bmatrix} x \\ y \\ 1 \end{bmatrix}$$</p>
            
            <p><strong>Reprojection Error:</strong></p>
            <p>$$\text{error} = ||HP-Q||_2$$</p>
            <p>This measures how well the homography maps points from image 1 to image 2.</p>
            
            <p><strong>Inlier Criterion:</strong></p>
            <p>$$\text{error} < \text{threshold} \quad $$</p>
        </div>

        <h5>Implementation Parameters and Configuration</h5>
        <p>
            The implementation exposes several configurable parameters for fine-tuning performance:
        </p>

        <ul>
            <li><strong>--iters (default 3000):</strong> Maximum RANSAC iterations. Higher values increase success probability but slow processing.</li>
            <li><strong>--thresh (default 3.0):</strong> Inlier threshold in pixels. Lower values are stricter but may reject valid matches.</li>
            <li><strong>--method (normalized/Ahb):</strong> Choose between normalized DLT or $Ah=b$ homography estimation methods.</li>
            <li><strong>--one_sided:</strong> Use reprojection error $||HP-Q||$ for faster computation (default behavior).</li>
        </ul>

        <h5>Mosaicing Integration</h5>
        <p>
            For automatic mosaicing, the estimated homography is integrated into a complete stitching pipeline:
        </p>

        <div class="matrix-display">
            <p><strong>Mosaicing Process:</strong></p>
            <ol>
                <li><strong>Reference Selection:</strong> Choose a reference image and set $H_{ref} = I$ (identity matrix)</li>
                <li><strong>Global Canvas:</strong> Compose each non-reference homography $H$ to the global canvas coordinate system</li>
                <li><strong>Canvas Bounds:</strong> Compute global canvas by warping all image corners and applying translation offset</li>
                <li><strong>Image Warping:</strong> Inverse-warp each image using bilinear interpolation (--nn option for nearest-neighbor)</li>
                <li><strong>Alpha Blending:</strong> Blend images using feathered alpha averaging where alpha falls off toward edges</li>
            </ol>
        </div>

        <h5>Quality Assessment and Visualization</h5>
        <p>
            The implementation provides comprehensive logging and visualization capabilities:
        </p>

        <ul>
            <li><strong>Inlier Statistics:</strong> Logs inlier counts, ratios, and convergence information for each image pair</li>
            <li><strong>Match Visualization:</strong> Saves per-pair inlier visualizations with green (inlier) vs red (outlier) matches</li>
            <li><strong>Performance Metrics:</strong> Tracks processing time, iteration counts, and success rates</li>
            <li><strong>Debug Output:</strong> Provides detailed information about homography quality and geometric consistency</li>
        </ul>

        <h4>Parameter Tuning and Effects</h4>
        <p>
            Through extensive experimentation, I discovered how different RANSAC parameters affect performance:
        </p>

        <ul>
            <li><strong>Threshold (5.0 pixels):</strong> Lower values (3-4 pixels) are stricter but may reject valid matches. Higher values (6-8 pixels) are more permissive but may accept outliers. 5.0 provides optimal balance.</li>
            <li><strong>Max iterations (1000):</strong> More iterations increase success probability but slow processing. 1000 iterations provide 99.9% confidence of finding correct model with 50% inlier ratio.</li>
            <li><strong>Min inliers (10):</strong> Minimum required for valid homography. Lower values (4-6) are more permissive but may accept poor models. Higher values (15-20) are more robust but may reject valid cases.</li>
            <li><strong>Early termination:</strong> Stop when inlier ratio exceeds 80% to speed up processing without sacrificing quality.</li>
        </ul>

        <h4>Manual vs Automatic Mosaicing Comparison</h4>
        <p>
            The following comparison demonstrates the progression from manual to automatic mosaicing techniques. 
            Part A used manual correspondence point selection, while Part B implements a fully automatic pipeline 
            using computer vision algorithms. Both approaches achieve high-quality results, but the automatic 
            method eliminates the need for human intervention while maintaining comparable accuracy.
        </p>

        <h5>Manual Mosaics (Part A)</h5>
        <div class="image-row">
            <figure>
                <img src="../results/mosaic_house_fixed.jpg" alt="House Manual Mosaic">
                <figcaption>House Building - Manual Mosaic<br>
                <small>6 manually selected correspondence points</small></figcaption>
            </figure>
            <figure>
                <img src="../resultsB/sproul_manual.png" alt="Sproul Manual Mosaic">
                <figcaption>Sproul Hall - Manual Mosaic<br>
                <small>6 manually selected correspondence points</small></figcaption>
            </figure>
            <figure>
                <img src="../resultsB/wheeler_manual.png" alt="Wheeler Manual Mosaic">
                <figcaption>Wheeler Hall - Manual Mosaic<br>
                <small>6 manually selected correspondence points</small></figcaption>
            </figure>
        </div>

        <h5>Automatic Mosaics (Part B)</h5>
        <div class="image-row">
            <figure>
                <img src="../resultsB/house_mosaic.jpg" alt="House Automatic Mosaic">
                <figcaption>House Building - Automatic Mosaic<br>
                <small>22 inliers, RANSAC homography</small></figcaption>
            </figure>
            <figure>
                <img src="../resultsB/sproul_mosaic.jpg" alt="Sproul Automatic Mosaic">
                <figcaption>Sproul Hall - Automatic Mosaic<br>
                <small>17 inliers, RANSAC homography</small></figcaption>
            </figure>
            <figure>
                <img src="../resultsB/wheeler_mosaic.jpg" alt="Wheeler Automatic Mosaic">
                <figcaption>Wheeler Hall - Automatic Mosaic<br>
                <small>13 inliers, RANSAC homography</small></figcaption>
            </figure>
        </div>

        <div class="info-box">
            <strong>Comparison Analysis:</strong> Both manual and automatic approaches produce high-quality mosaics, 
            but the automatic method offers several advantages: (1) <strong>Scalability:</strong> Can process any 
            number of image pairs without manual intervention, (2) <strong>Consistency:</strong> Eliminates human 
            bias in correspondence selection, (3) <strong>Robustness:</strong> RANSAC automatically handles outliers, 
            and (4) <strong>Speed:</strong> Processes images much faster than manual selection. The automatic 
            system achieves good inlier ratios: House (73.3%), Wheeler (86.7%), and Sproul (60.7%), demonstrating 
            robust feature matching and homography estimation. Additionally, the automatic 
            mosaics exhibit clearer blending with less blurriness in the stitched regions compared to the manual version. 
            This improved quality results from more precise homography estimation using 15-30 correspondences (vs 4-8 manual), 
            sub-pixel accuracy in feature detection, and consistent feather alpha blending parameters without human bias.
        </div>

  

        <div class="info-box">
            <strong>RANSAC Success and Quality Metrics:</strong> The automatic homography estimation achieved excellent results across all image pairs:
            <ul>
                <li><strong>House Building:</strong> 22 inliers out of 30 matches (73.3% inlier ratio) - Good quality</li>
                <li><strong>Wheeler Hall:</strong> 13 inliers out of 15 matches (86.7% inlier ratio) - Excellent quality</li>
                <li><strong>Sproul Hall:</strong> 17 inliers out of 28 matches (60.7% inlier ratio) - Good quality</li>
            </ul>
            These results demonstrate that RANSAC successfully identified and rejected outlier correspondences 
            while preserving the geometrically consistent matches needed for accurate homography estimation. 
            The good inlier ratios (60.7-86.7%) indicate robust feature matching quality and reliable homography estimation.
        </div>

  

        <h4>Key Advantages of Automatic Approach</h4>
        <ul>
            <li><strong>Scalability:</strong> Can process any number of image pairs without manual point selection</li>
            <li><strong>Consistency:</strong> Eliminates human bias and selection errors in correspondence points</li>
            <li><strong>Robustness:</strong> RANSAC automatically handles outliers and noisy correspondences</li>
            <li><strong>Speed:</strong> Once implemented, processes images much faster than manual selection</li>
            <li><strong>Quality:</strong> Achieves 60.7-86.7% inlier ratios, indicating good matching quality</li>
        </ul>


        <!-- Technical Insights -->
        <h2>Technical Insights and Challenges</h2>
        
        <h3>Key Learnings from Manual Mosaicing</h3>
        <ul>
            <li><strong>Correspondence Point Quality:</strong> The accuracy of manual point selection directly 
            impacts alignment quality. I learned that corner features and high-contrast edges provide more reliable 
            correspondences than low-contrast or poorly-defined features. Small errors (±2-3 pixels) in point 
            selection propagate through the homography computation but are mitigated by using overdetermined systems.</li>
         
            <li><strong>Blending Techniques:</strong> Simple averaging in overlap regions produces visible seams 
            even with perfect geometric alignment. Alpha feathering with distance-based weights creates professional 
            results by accounting for exposure variations, vignetting, and minor alignment errors.</li>

            <li><strong>Mathematical Rigor:</strong> The SVD-based solution for homography estimation provides 
            robust results even with noisy correspondences. The overdetermined system (12 equations, 8 unknowns) 
            provides redundancy that improves accuracy and handles minor selection errors.</li>

            <li><strong>Interpolation Quality:</strong> Bilinear interpolation is essential for professional-quality 
            results. Nearest neighbor interpolation, while faster, produces visible pixelation artifacts that 
            degrade the final mosaic quality significantly.</li>
        </ul>

        <h3>Challenges Encountered and Solutions</h3>
        <ul>
            <li><strong>Homography Direction Ambiguity:</strong> The most significant challenge was determining 
            the correct direction for the homography. Initially, H mapped Image 2 to the left of Image 1 instead 
            of to the right. This occurred because the homography maps points from Image 2 → Image 1, but for 
            mosaic creation we needed Image 1 → Image 2 mapping. 
            <br><strong>Solution:</strong> Implemented automatic detection by transforming Image 2's center point 
            and checking if it lands left or right of Image 1's center. If left, the code automatically uses H⁻¹. 
            This diagnostic approach (shown in console output: "H maps Image 2 to the LEFT... Using H_inv") made 
            the pipeline robust to correspondence point ordering.</li>
            
            <li><strong>Canvas Bounds Computation:</strong> Computing correct global canvas bounds required careful 
            consideration of how all image corners transform. Initially attempted to place images sequentially, 
            which caused clipping when warped images extended into negative coordinates.
            <br><strong>Solution:</strong> Project all corners of all images through their homographies first, 
            find the global min/max, then apply consistent translation offsets to place everything in positive 
            coordinates. This ensures the entire panorama fits within the canvas.</li>
            
            <li><strong>Alpha Map Warping:</strong> Initially applied feathering to already-warped images, which 
            created incorrect blend weights due to the warping distorting the distance-to-edge metric.
            <br><strong>Solution:</strong> Create alpha maps on the original images before warping, then warp 
            the alpha maps using the same homography. This preserves the correct geometric relationship between 
            pixel positions and their edge distances.</li>


        </ul>

        <!-- Conclusion -->
        <h2>Conclusion</h2>
        <p>
            This project successfully implemented a complete image mosaicing pipeline from first principles, 
            demonstrating both theoretical understanding and practical implementation skills. The resulting 
            panoramas of Sproul Hall, Wheeler Hall, and House Building showcase seamless stitching that rivals 
            commercial panorama software, achieved through careful attention to mathematical rigor, algorithmic 
            correctness, and sophisticated blending techniques.
        </p>
        <p>
            The project demonstrates the complete progression from manual to automatic computer vision techniques:
            <strong>Part A</strong> established the mathematical foundation through manual correspondence selection 
            and homography computation, while <strong>Part B</strong> implemented a fully automatic pipeline using 
            Harris corner detection, feature descriptors, intelligent matching, and RANSAC-based homography estimation. 
            The automatic system achieves excellent results with 86.7-90% inlier ratios, demonstrating the power 
            of robust statistical methods in computer vision.
        </p>
        <p>
            The experience reinforced several key lessons applicable to computer vision broadly: (1) geometric 
            transformations require careful coordinate system management, (2) mathematical elegance (SVD-based 
            least squares) often provides the most robust practical solutions, (3) the final 10% of quality 
            (seamless blending) requires as much effort as the first 90% (geometric alignment), (4) debugging 
            and diagnostic tools (visualization, automatic checks) are essential for building reliable vision systems, 
            and (5) robust statistical methods like RANSAC are crucial for handling real-world data with outliers 
            and noise.
        </p>



        <div class="footer">
            <p><strong>CS180 Project 3: Image Mosaics and Homographies</strong></p>
            <p>Part A: Manual Mosaicing | Part B: Automatic Mosaicing</p>
            <p>Yuxuan Cai | UC Berkeley | Fall 2025</p>
        
        </div>
    </div>
</body>
</html>